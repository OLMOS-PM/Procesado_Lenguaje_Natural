{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "re9Lt_eGi1R-"
   },
   "source": [
    "# Procesamiento de Lenguaje Natural\n",
    "\n",
    "\n",
    "**Pablo Martínez Olmos, Vanessa Gómez Verdejo, Emilio Parrado Hernández**\n",
    "\n",
    "Departamento de Teoría de la Señal y Comunicaciones\n",
    "\n",
    "**Universidad Carlos III de Madrid**\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UZTTwuuIN0MH"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "# Figures plotted inside the notebook\n",
    "%config InlineBackend.figure_format = 'svg'  \n",
    "# High quality figures\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWvDrfYgN13e"
   },
   "source": [
    "# Introducción\n",
    "\n",
    "Hasta ahora hemos estado trabajando con datos de tipo  numérico o categórico. En esta sesión vamos a ver cómo trabajar con nuestros datos cuando éstos son cadenas de texto. A diferencia de los datos categóricos, en los que tenemos cadenas de texto asociadas a diferentes categorías y que podemos codificar fácilmente (por ejemplo, con un one-hot-encoding), cuando hablamos aquí de información textual nos referimos a frases, documentos y/o corpus de datos con estructura mucho más compleja. Idealmente, a partir de estos datos textuales tenemos que extraer la información necesaria (a poder ser incluyendo contenido semántico) y vectorizarla adecuadamente para poder utilizar o usar modelos de aprendizaje a partir de ella.\n",
    "\n",
    "En general, gran parte de la forma en que nos comunicamos hoy en día es a través de texto escrito, ya sea en servicios de mensajería, medios sociales y/o correo electrónico. Así, por ejemplo, en servicios/aplicaciones como TripAdvisor, Booking, Amazon, etc., los usuarios escriben reseñas de restaurantes/negocios, hoteles, productos para compartir sus opiniones sobre su experiencia. Estas reseñas, todas escritas en formato de texto, contienen una gran cantidad de información que sería útil responder preguntas relevantes para el negocio usando métodos de aprendizaje automático, por ejemplo, para predecir el mejor restaurante en una determinada zona. \n",
    "\n",
    "Este tipo de tarea (preprocesado) se denomina **procesamiento del lenguaje natural** (Natural Language Processing, NLP).\n",
    "El NLP es un subcampo de la lingüística, la informática y la inteligencia artificial que se ocupa de las interacciones entre los ordenadores (o procesadores) y el lenguaje humano; en particular engloba un conjunto de técnicas para permitir que los ordenadores procesen y analicen grandes cantidades de texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3v4bBAVOM2z"
   },
   "source": [
    "# Pipeline para el procesado de texto \n",
    "\n",
    "Como sabemos, los algoritmos de ML procesan números, no palabras, por lo que necesitamos transformar el texto en números significativos que contengan la información relevante de los documentos. Este proceso de convesión de texto a números es lo que llamaremos **vectorización**. \n",
    "\n",
    "No obstante, para tener una representación útil, se requieren normalmente algunos pasos de **preprocesamiento** previo que limpien y homogenizen los documentos: tokenización, eliminación de *stop-words*, lematización, etc.\n",
    "La siguiente figura muestra los diferentes pasos que debemos seguir para procesar nuestros documentos hasta poder ser utilizados por nuestro modelo de aprendizaje:\n",
    "\n",
    "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/BBVA/NLP/PipelineNLP.png\" width=\"80%\"> \n",
    "\n",
    "A lo largo de este notebook, veremos las herramientas que tenemos disponibles en Python para llevar a cabo todos estos pasos. Concretamente, nos centraremos en el uso de dos librerias:\n",
    "* [NLTK, Natural Language ToolKit](https://www.nltk.org/). Esta libreria es una excelente biblioteca de NLP escrita en Python por expertos tanto del mundo académico como de la industria. NLTK permite crear aplicaciones con datos textuales rápidamente, ya que proporciona un conjunto de clases básicas para trabajar con corpus de datos, incluyendo colecciones de textos (corpus), listas de palabras clave, clases para representar y operar con datos de tipo texto (documentos, frases, palabras, ...) y funciones para realizar tareas comunes de NLP (conversión a token, conteo de palabras, ...). Por lo que va a ser de gran ayuda para el preprocesado de los documentos.\n",
    "\n",
    "\n",
    "* [SpaCy](https://spacy.io/) es una biblioteca de código abierto para procesamiento avanzado del lenguaje natural en Python. SpaCy está diseñado específicamente para su uso en producción. A diferencia de NLTK, SpaCy tiene una estructura orientada a objetos. Por ejemplo, al tokenizar texto, cada token es un objeto con atributos y propiedades específicas. SpaCy admite más de 64 idiomas, incluyendo modelos estadísticos ya entrenados para [17 de ellos](https://spacy.io/usage/models) (incluyendo modelos basados en [transformers](https://spacy.io/usage/v3), la última revolución en NLP).\n",
    "\n",
    "\n",
    "* [Gensim](https://pypi.org/project/gensim/) es otra librería de Python para el modelado por temáticas (*topic modeling*), la indexación de documentos y tareas de recuperación de la información para documentos. Está diseñada para operar con grandes cantidades de información (con implementaciones eficientes y paralelizables/distribuidas) y nos va a ser de gran ayuda para la vectorización de nuestros corpus de datos una vez preprocesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Base de datos \n",
    "\n",
    "NLTK incluye diferentes corpus de datos para probar nuestras herramientas. Podemos encontrar información sobre todos ellos en [corpus NLTK] (https://www.nltk.org/book/ch02.html).\n",
    "\n",
    "A pesar de que podemos usar NLTK para realizar el preprocesamiento de texto, usaremos [spaCy](https://spacy.io/) para tal fin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El objeto CorpusReader\n",
    "Para empezar a trabajar vamos a utilizar el corpus **movie_reviews**, uno de los corpus de datos incluidos en NLTK y formado por 2000 documentos de texto con reseñas de diferentes películas donde además se indica si estas reseñas son positivas o negativas.\n",
    "\n",
    "La siguiente celda de código muestra cómo cargar el corpus...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/olmos/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<CategorizedPlaintextCorpusReader in '/Users/olmos/nltk_data/corpora/movie_reviews'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "nltk.download('movie_reviews')\n",
    "movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al cargar el corpus, se genera un objeto de tipo `CorpusReader`, denominado `movie_reviews`, con el contenido del corpus. Dado que un corpus es una colección de documentos/textos, podemos ver qué documentos componen este corpus usando la función `.fileids()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_reviews.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg/cv000_29416.txt',\n",
       " 'neg/cv001_19502.txt',\n",
       " 'neg/cv002_17424.txt',\n",
       " 'neg/cv003_12683.txt',\n",
       " 'neg/cv004_12641.txt',\n",
       " 'neg/cv005_29357.txt',\n",
       " 'neg/cv006_17022.txt',\n",
       " 'neg/cv007_4992.txt',\n",
       " 'neg/cv008_29326.txt',\n",
       " 'neg/cv009_29417.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.fileids()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos ver las categorías de este problema con el atributo `.categories()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories('neg/cv000_29416.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos, podemos acceder a un **documento** específico en el corpus y extraer su contenido sin procesar con la función `.raw()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that's exactly how long the movie felt to me . \n",
      "there weren't even nine laughs in nine months . \n",
      "it's a terrible mess of a movie starring a terrible mess of a man , mr . hugh grant , a huge dork . \n",
      "it's not the whole oral-sex/prostitution thing ( referring to grant , not me ) that bugs me , it's the fact that grant is annoying . \n",
      "not just adam sandler-annoying , we're talking jim carrey-annoying . \n",
      "since when do eye flutters and nervous smiles pass for acting ? \n",
      "but , on the other hand , since when do really bad slapstick ( a fistfight in the delivery room culminating in grant's head in joan cusack's lap--a scene he paid $60 to have included in the movie ) and obscene double entendres ( robin williams , the obstetrician , tells grant's pregnant girlfriend she has \" a big pussy , \" referring of course to the size of the cat hairs on her coat , but nonetheless , grant paid $60 to have the exchange included in the movie ) pass for comedy ? \n",
      "nine months is a predictable cookie-cutter movie with no originality in humor or plot . \n",
      "hugh grant plays a successful child psychiatrist . \n",
      "why a child psychologist ? \n",
      "so the scriptwriters could inject the following unfunny exchange : \n",
      "kid : my dad's an asshole . \n",
      "grant ( flutters eyelashes , offers a nervous smile , then responds in his annoying english accent and i-think-i-actually-have- talent attitude ) : could you possibly elaborate on that ? \n",
      "kid : my dad's a _huge_ asshole . \n",
      "more like a hugh asshole , but that's beside the point , which is : nine months includes too many needlessly stupid jokes that get laughs from the ten year olds in the audience while everyone else shakes his or her head in disbelief . \n",
      "so , anyway , grant finds out his girlfriend is pregnant and does his usual reaction ( fluttered eyelashes , nervous smiles ) . \n",
      "this paves the way for every possible pregnancy/child birth gag in the book , especially since grant's equally annoying friend's wife is also pregnant . \n",
      "the annoying friend is played by tom arnold , who provides most of the cacophonous slapstick , none of which is funny , such as a scene where arnold beats up a costumed \" arnie the dinosaur \" ( you draw your own parallels on that one ) in a toy store . \n",
      "the only interesting character in the movie is played by jeff goldblum , who should have hid himself away somewhere after the dreadful hideaway , as an artist with a fear of ( and simultaneous longing for ) commitment . \n",
      "not even robin williams , who plays a russian doctor who has recently decided to switch from veterinary medicine to obstetrics , has much humor . \n",
      "his is a one-joke character-- the old foreign-guy-who-mispronounces-english stereotype ( did someone say yakov smirnov ? \n",
      "that's my favorite vodka , by the way ) , hence the line \" now it's time to take a look at your volvo , \" another nasty but unamusing joke , except this one goes right over the ten year olds' heads , while the adults simultaneously groan . \n",
      "nine months is a complete failure , low on laughs and intelligence and high on loud , unfunny slapstick , failed jokes and other uninspired lunacy . \n",
      "hugh grant's sunset boulevard arrest ( please , no caught-with-his-pants-down jokes ) may bring more people into the theaters , but they certainly won't leave with a smile on their faces , not after 90 minutes of grant's nervous smiles . \n",
      "everything in the movie is so forced , so unauthentic that anyone with an i . q . \n",
      "over 80 ( sorry , hugh ) will know they wasted their money on an unfulfilled desire . \n",
      "but at least they didn't spend 60 bucks for it . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_text = movie_reviews.raw('neg/cv007_4992.txt')\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora en la variable `raw_text` tenemos un array de caracteres o *string* (con todas las funciones de los tipos *string*). Por ejemplo, podemos ver los primeros 40 caracteres de este documento como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "that's exactly how long the movie felt t\n",
      "\n",
      " The total number of characters in the document is 3554\n"
     ]
    }
   ],
   "source": [
    "print(type(raw_text))\n",
    "\n",
    "print(raw_text[:40])\n",
    "\n",
    "print('\\n The total number of characters in the document is %d' %(len(raw_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xojODsSNb4G"
   },
   "source": [
    "# 2. Preprocesado del corpus\n",
    "\n",
    "Antes de transformar los datos de entrada de texto en una representación vectorial, necesitamos estructurar y limpiar el texto, y conservar toda la información que permita capturar el contenido semántico del corpus.\n",
    "\n",
    "Para ello, el procesado típico de NLP aplica los siguientes pasos:\n",
    "\n",
    "1. Tokenización\n",
    "2. Homogeneización\n",
    "3. Limpieza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBOayTbJNb4G"
   },
   "source": [
    "## 2.1. Tokenization\n",
    "\n",
    "**Tokenización** es el proceso de dividir el texto dado en piezas más pequeñas llamadas tokens. Las palabras, los números, los signos de puntuación y otros pueden ser considerados como tokens.\n",
    "\n",
    "Ya hemos visto que el objeto `CorpusReader` incluye funciones para dividir el corpus en frases o palabras. Pero NLTK incluye también funciones genéricas para hacer estas operaciones sobre cualquier cadena de texto. En concreto, tiene dos funciones:\n",
    "- `sent_tokenize`: es un tokenizador de frases. Este tokenizador divide un texto en una lista de oraciones. Para decidir dónde empieza o acaba una frase NLTK tiene un modelo pre-entrenado para el idioma específico en el que estemos trabajando. Este modelo lo hemos cargado al principio con `nltk.download('punkt')`.\n",
    "- `word_tokenize`/`wordpunct_tokenize`:  Divide un texto en palabras u otros caracteres individuales cómo pueden ser signos de puntuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8T-OErnWNb4H"
   },
   "source": [
    "## 2.2. Homogeneización\n",
    "\n",
    "Al observar los tokens del corpus podemos ver que hay muchos tokens con algunas letras en mayúsculas y otras en minúsculas, el mismo token unas veces aparece en singular y otras en plural, o el mismo verbo que aparece en diferentes tiempos verbales. Para analizar semánticamente el texto, nos interesa  **homogeneizar** las palabras que formalmente son diferentes pero tienen el mismo significado.\n",
    "\n",
    "El proceso habitual de homogeneización consiste en los siguientes pasos:\n",
    "\n",
    "1. Eliminación de las mayúsculas y caracteres no alfanuméricos: de este modo los caracteres alfabéticos en mayúsculas se transformarán en sus correspondientes caracteres en minúsculas y  se eliminarán los caracteres no alfanuméricos, por ejemplo, los signos de puntuación. \n",
    "\n",
    "2. Stemming/Lematización: eliminar las terminaciones de las palabras para preservar su raíz de las palabras e ignorar la información gramatical (eliminamos marcas de plurales, género, conjugaciones verbales, ...).\n",
    "\n",
    "3. Eliminar **stopping words** (palabras que no son en general informativas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6ozm2RjKLap"
   },
   "source": [
    "**Stemming and Lemmatización**\n",
    "\n",
    "En el lenguaje común, las palabras pueden tomar diferentes formas indicando género, cantidad, tiempo (en el caso de los verbos), formas concretas para nombres/adjetivos o adverbios, ... Para muchas aplicaciones, es útil normalizar estas formas en alguna palabra canónica que facilite su análisis. Hay dos maneras de realizar este proceso:\n",
    "\n",
    "1. El proceso de **stemming** reduce las palabras a su base o raíz \n",
    "\n",
    "      running --> run\n",
    "\n",
    "      flowers --> flower\n",
    "\n",
    "  Para poder hacer esta transformación necesitamos librerías específicas que tienen almacenadas para el vocabulario de cada idioma las raices de dicho vocabulario y hacen esta conversión. \n",
    "\n",
    "    \n",
    "2. El objetivo de la **lematización**, al igual que el stemmer, es reducir las formas inflexionales a una forma base común. A diferencia del steming, la lematización no se limita a cortar las inflexiones. En su lugar, utiliza bases de conocimiento léxico para obtener las formas básicas correctas de las palabras. \n",
    "\n",
    "    women   --> woman\n",
    "\n",
    "    foxes   --> fox\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPcY6x5HsjzP"
   },
   "source": [
    "Una de las ventajas de la lematización es que el resultado sigue siendo una palabra, lo que es más aconsejable para la presentación de los resultados del procesado de textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCd3LMgCH8Ra"
   },
   "source": [
    "**Eliminar palabras irrelevantes**\n",
    "\n",
    "El último paso del preprocesado consiste en eliminar las palabras irrelevantes o **stop words** de los documentos. Las **stop words** son las palabras más comunes en un idioma como \"el\", \"a\", \"sobre\", \"es\", \"todo\". Estas palabras no tienen un significado importante y normalmente se eliminan de los textos. Para aplicar este proceso, se cargan librerías específicas que contienen este listado de palabras por cada idioma.\n",
    "\n",
    "Además, este paso suele utilizarse para eliminar las marcas de puntaución (',', '.', '?', ....), para lo que también tenemos que cargar otro módulo con los signos de puntuación. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado de texto con spaCy\n",
    "\n",
    "[spaCy](https://spacy.io/) es una biblioteca gratuita de código abierto para el procesamiento avanzado del lenguaje natural en Python. spaCy está diseñado específicamente para uso en producción. A diferencia de NLTK, spaCy sigue una orientación de objetos. Por ejemplo, cuando tokenizamos un texto, cada token es un objeto con atributos y propiedades específicas.\n",
    "\n",
    "Spacy da soporte para más de 64 idiomas, incluyendo modelos estadísticos ya entrenados para [17 de ellos](https://spacy.io/usage/models) (incluyendo word embeddings y modelos basados en [transformers](https://spacy.io/usage/v3), la última revolución en NLP).\n",
    "\n",
    "\n",
    "Puesto que en esta sesión sólo vamos a cubrir algunos aspectos básicos de spaCy, en los siguientes recursos podéis encontrar material adicional:\n",
    "\n",
    "- [spaCy 101 course](https://spacy.io/usage/spacy-101)\n",
    "- [Advanced Tutorial](https://course.spacy.io/en/)\n",
    "\n",
    "\n",
    "El procesado de texto con spaCy es sencillo. Cargaremos un modelo pre-entrenado para un determinado idioma, y pasamos cualquier texto a procesar. spaCy ejecutará una serie de procesos (pipeline) sobre el mismo y  devolverá un objeto tipo `doc`.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://spacy.io/images/architecture.svg' width=\"800\"></img>\n",
    "<figcaption>From Spacy documentation</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "La arquitectura básica en spaCy es la siguiente:\n",
    "\n",
    "   - `Language`: se determina al cargar el modelo y el pipeline de procesos asociados. Trasforma texto en objectos spaCy.\n",
    "   - `Doc`: Secuencia iterable de tokens. \n",
    "   - `Vocab`: Diccionario asociado al modelo.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente código, descargamos e importamos uno de los [modelos estadísticos pre-entrenados para idioma inglés](https://spacy.io/models/en)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para forzar spacy 3.0 (con colab se abre 2.2.X)\n",
    "#!pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: setuptools in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: jinja2 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Descargamos el modelo\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargamos el modelo\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El **acceso al vocabulario** en modo se hace a través del atributo `.vocab.strings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del diccionario es de 83839 palabras\n",
      "['\\t', '\\n', ' ', '  ', '!', '!!', '!!!', '!!!!', '!!!!!!!!!!!!!!!!', '!!!!.', '!!.', '!!?', '!!??', '!*', '!.', '!?', '!??', '\"', '\"\"', '#', \"##'s\", \"##'x\", \"#'s\", '#15', '#^%', '#dd', '$', '$19', '$Whose', '$Xxxxx', '$whose', '$xxxx', '%', '%-3', '%ach', '%ah', '%eh', '%er', '%ha', '%hm', '%huh', '%mm', '%oof', '%pw', '%uh', '%um', '%xx', '%xxx', '&', '&#', '&G.', '&L.', '&Ls', '&M.', '&P.', '&SA', '&T.', '&ex', '&in', '&ls', '&of', '&on', '&sa', '&the', '&to', '&uh', '&von', '&xx', '&xxx', \"'\", \"''\", \"''It\", \"''Xx\", \"''it\", \"''xx\", \"'-(\", \"'-)\", \"'03\", \"'07\", \"'20s\", \"'30s\", \"'40s\", \"'45\", \"'46\", \"'50s\", \"'60s\", \"'67\", \"'68\", \"'69\", \"'70's\", \"'70s\", \"'71\", \"'73\", \"'74\", \"'76\", \"'78\", \"'80\", \"'80's\", \"'80s\", \"'82\", \"'86\", \"'89\", \"'90's\", \"'90s\", \"'91\", \"'94\", \"'96\", \"'97\", \"'98\", \"'99\", \"'Arabi\", \"'Cause\", \"'Connery\", \"'Cos\", \"'Coz\", \"'Cuz\", \"'Id\", \"'Il\", \"'It\", \"'N\", \"'Nita\", \"'S\", \"'T\", \"'X\", \"'Xx\", \"'Xxx\", \"'Xxxx\", \"'Xxxxx\", \"'ai\", \"'al\", \"'am\", \"'amour\", \"'an\", \"'ao\", \"'arabi\", \"'bout\", \"'cause\", \"'connery\", \"'cos\", \"'coz\", \"'cuz\", \"'d\", \"'d.\", \"'dd\", \"'dd'x\", \"'ddx\", \"'droid\", \"'em\", \"'en\", \"'er\", \"'id\", \"'il\", \"'in\", \"'it\", \"'ll\", \"'m\", \"'ma\", \"'n\", \"'n'\", \"'nita\", \"'ns\", \"'nt\", \"'nuff\", \"'re\", \"'recg\", \"'s\", \"'s**\", \"'s_\", \"'t\", \"'til\", \"'ts\", \"'uh\", \"'ve\", \"'x\", \"'x'\", \"'x**\", \"'xx\", \"'xxx\", \"'xxxx\", \"'y\", '(', '(((', '(*>', '(*_*)', '(-8', '(-:', '(-;', '(-_-)', '(-d', '(._.)', '(02)', '(:', '(;', '(=', '(>_<)', '(AM', '(^_^)', '(c)?Ìö]o?', '(c)?ìö]o?', '(c)x']\n"
     ]
    }
   ],
   "source": [
    "lista_vocab = list(nlp.vocab.strings)\n",
    "\n",
    "print(\"El tamaño del diccionario es de {} palabras\".format(len(lista_vocab)))\n",
    "\n",
    "#Las 200 primeras\n",
    "print(lista_vocab[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el vocabulario incluye ya emoticonos codificados en modo texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay un total de 73384 palabras con caracterés alfabéticos. \n",
      " Las 20 primeras son ...\n",
      "['A', 'AA', 'AAA', 'AAC', 'AAV', 'AB', 'ABA', 'ABB', 'ABBA', 'ABBIE', 'ABC', 'ABCs', 'ABD', 'ABM', 'ABORTION', 'ABS', 'ABUSE', 'AC', 'ACC', 'ACCEPTANCES']\n"
     ]
    }
   ],
   "source": [
    "no_alpha_words = [word for word in nlp.vocab.strings if word.isalpha() is True]\n",
    "\n",
    "print(f'Hay un total de {len(no_alpha_words)} palabras con caracterés alfabéticos. \\n Las 20 primeras son ...')\n",
    "\n",
    "print(no_alpha_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### objetos de texto en SpaCy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, vamos a utilizar el pipeline que hemos cargado para analizar un texto ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan’s economy, struggling to emerge from the pandemic, is sagging under rising food and energy costs and a weak yen\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Japan’s economy, struggling to emerge from the pandemic, is sagging under rising food and energy costs and a weak yen'\n",
    "\n",
    "print(sentence)\n",
    "\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc` es un objeto iterable, compuesto por objetos tipo [`token`](https://spacy.io/api/token). En el siguiente bucle imprimimos algunas de las propiedades de dichos tokens determinadas por el pipeline que hemos cargado, incluyendo el POS tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan Japan NNP True False False\n",
      "*****\n",
      "’s ’s POS False True False\n",
      "*****\n",
      "economy economy NN True False False\n",
      "*****\n",
      ", , , False False True\n",
      "*****\n",
      "struggling struggle VBG True False False\n",
      "*****\n",
      "to to TO True True False\n",
      "*****\n",
      "emerge emerge VB True False False\n",
      "*****\n",
      "from from IN True True False\n",
      "*****\n",
      "the the DT True True False\n",
      "*****\n",
      "pandemic pandemic NN True False False\n",
      "*****\n",
      ", , , False False True\n",
      "*****\n",
      "is be VBZ True True False\n",
      "*****\n",
      "sagging sag VBG True False False\n",
      "*****\n",
      "under under IN True True False\n",
      "*****\n",
      "rising rise VBG True False False\n",
      "*****\n",
      "food food NN True False False\n",
      "*****\n",
      "and and CC True True False\n",
      "*****\n",
      "energy energy NN True False False\n",
      "*****\n",
      "costs cost NNS True False False\n",
      "*****\n",
      "and and CC True True False\n",
      "*****\n",
      "a a DT True True False\n",
      "*****\n",
      "weak weak JJ True False False\n",
      "*****\n",
      "yen yen NN True False False\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.tag_, token.is_alpha, token.is_stop,token.is_punct)\n",
    "    print('*****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En forma de tabla ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Word_Lemma</th>\n",
       "      <th>POS tag</th>\n",
       "      <th>Is alpha</th>\n",
       "      <th>Is stopword</th>\n",
       "      <th>Is punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NNP</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>’s</td>\n",
       "      <td>’s</td>\n",
       "      <td>POS</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>economy</td>\n",
       "      <td>economy</td>\n",
       "      <td>NN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>struggling</td>\n",
       "      <td>struggle</td>\n",
       "      <td>VBG</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>emerge</td>\n",
       "      <td>emerge</td>\n",
       "      <td>VB</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>from</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pandemic</td>\n",
       "      <td>pandemic</td>\n",
       "      <td>NN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sagging</td>\n",
       "      <td>sag</td>\n",
       "      <td>VBG</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>under</td>\n",
       "      <td>under</td>\n",
       "      <td>IN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rising</td>\n",
       "      <td>rise</td>\n",
       "      <td>VBG</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>food</td>\n",
       "      <td>food</td>\n",
       "      <td>NN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>energy</td>\n",
       "      <td>energy</td>\n",
       "      <td>NN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>costs</td>\n",
       "      <td>cost</td>\n",
       "      <td>NNS</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>weak</td>\n",
       "      <td>weak</td>\n",
       "      <td>JJ</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>yen</td>\n",
       "      <td>yen</td>\n",
       "      <td>NN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word Word_Lemma POS tag  Is alpha  Is stopword  Is punct\n",
       "0        Japan      Japan     NNP      True        False     False\n",
       "1           ’s         ’s     POS     False         True     False\n",
       "2      economy    economy      NN      True        False     False\n",
       "3            ,          ,       ,     False        False      True\n",
       "4   struggling   struggle     VBG      True        False     False\n",
       "5           to         to      TO      True         True     False\n",
       "6       emerge     emerge      VB      True        False     False\n",
       "7         from       from      IN      True         True     False\n",
       "8          the        the      DT      True         True     False\n",
       "9     pandemic   pandemic      NN      True        False     False\n",
       "10           ,          ,       ,     False        False      True\n",
       "11          is         be     VBZ      True         True     False\n",
       "12     sagging        sag     VBG      True        False     False\n",
       "13       under      under      IN      True         True     False\n",
       "14      rising       rise     VBG      True        False     False\n",
       "15        food       food      NN      True        False     False\n",
       "16         and        and      CC      True         True     False\n",
       "17      energy     energy      NN      True        False     False\n",
       "18       costs       cost     NNS      True        False     False\n",
       "19         and        and      CC      True         True     False\n",
       "20           a          a      DT      True         True     False\n",
       "21        weak       weak      JJ      True        False     False\n",
       "22         yen        yen      NN      True        False     False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spacy_pos_tagged = [(token.text, token.lemma_, token.tag_,token.is_alpha, token.is_stop,token.is_punct) for token in doc]\n",
    "\n",
    "pd.DataFrame(spacy_pos_tagged, columns=['Word','Word_Lemma','POS tag','Is alpha','Is stopword','Is punct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con `spacy.explain()` podemos obtener una descripción de los distintos tags ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'verb, non-3rd person singular present'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"VBP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjective (English), other noun-modifier (Chinese)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"JJ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo referente a **stopping words**, spaCy incluye una amplia lista (326 elementos en idioma inglés) que podemos personalizar para nuestra propia aplicación si es necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['once', 'about', 'ever', 'go', 'neither', 'its', 'yourselves', 'itself', 'done', 'by', 'say', 'hereafter', 'will', 'eleven', 'alone', 'hereupon', 'than', 'wherein', 'here', 'part']\n"
     ]
    }
   ],
   "source": [
    "spacy_stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "#Printing the total number of stop words:\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "\n",
    "#Printing first ten stop words:\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 327\n",
      "Number of stop words: 326\n"
     ]
    }
   ],
   "source": [
    "# Añadir una palabra al conjunto de stopping words\n",
    "\n",
    "nlp.Defaults.stop_words.add(\"my_new_stopword\")\n",
    "\n",
    "print('Number of stop words: %d' % len(nlp.Defaults.stop_words))\n",
    "\n",
    "# Quitar una palabra del conjunto de stopping words\n",
    "\n",
    "nlp.Defaults.stop_words.remove(\"my_new_stopword\")\n",
    "\n",
    "print('Number of stop words: %d' % len(nlp.Defaults.stop_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy para idioma español\n",
    "\n",
    "Tal y como hemos comentado, Spacy proporciona modelos pre-entrenados para trabajar con idioma español. Todos han sido entrenados en la base de datos anotada [Ancora](http://clic.ub.edu/corpus/). Este corpus contiene 500.000 textos  periodísticos publicados en medios españoles. \n",
    "\n",
    "Es recomendable mirar la siguiente [documentación](https://spacy.io/models/es).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.2.0/es_core_news_sm-3.2.0-py3-none-any.whl (14.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.0 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from es-core-news-sm==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (0.7.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/olmos/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.1.1)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del diccionario es de 181846 palabras. Las primeras 100 son \n",
      "\n",
      "['\\t', '\\n', ' ', '  ', ' el', ' tú', ' yo', ' él', '!', '\"', '\"\"\"', '\"Caño\"', '\"El', '\"Guga\"', '\"Steel\"', '\"Torito\"', '\"Tucho\"', '\"Xx', '\"Xxxx\"', '\"Xxxxx\"', '\"añicos', '\"caño\"', '\"el', '\"guga\"', '\"steel\"', '\"torito\"', '\"tucho\"', '\"write', '\"xx', '\"xxxx', '\"xxxx\"', '#', '$', '%', '&', \"'\", \"' matar tú '\", \"' xxxx xx '\", \"''\", \"'-(\", \"'-)\", \"'13\", \"'23\", \"'70\", \"'92\", \"'96\", \"'Arteaga'\", \"'Catanha'\", \"'Erika'\", \"'Guti'\", \"'Or\", \"'Xxxx'\", \"'Xxxxx'\", \"'an\", \"'arteaga'\", \"'catanha'\", \"'d\", \"'di\", \"'em\", \"'er\", \"'erika'\", \"'ev\", \"'excelencia'\", \"'guti'\", \"'hooligans'\", \"'il\", \"'in\", \"'ir\", \"'ll\", \"'m\", \"'matarte'\", \"'mystes'\", \"'or\", \"'re\", \"'s\", \"'savoir\", \"'síndrome\", \"'ve\", \"'wa\", \"'x\", \"'xx\", \"'xxxx\", \"'xxxx'\", \"'ya\", \"'нs\", '(', '(((', '(*>', '(*_*)', '(-8', '(-:', '(-;', '(-_-)', '(-d', '(._.)', '(:', '(;', '(=', '(>_<)', '(^_^)']\n",
      "\n",
      "Hay un total de 551 stopping words en el modelo\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "lista_vocab = list(nlp.vocab.strings)\n",
    "\n",
    "print(\"El tamaño del diccionario es de {} palabras. Las primeras 100 son \\n\".format(len(lista_vocab)))\n",
    "\n",
    "print(lista_vocab[:100])\n",
    "\n",
    "#Printing the total number of stop words:\n",
    "print('\\nHay un total de {0} stopping words en el modelo'.format(len(spacy.lang.es.stop_words.STOP_WORDS)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de SpaCy para el preprocesamiento de texto\n",
    "\n",
    "Como hemos visto, usando SpaCy, podemos obtener directamente a los lemas de cada token junto con algunos atributos para indicar si el token es una palabra vacía (.is_stop), alfanumérico (.is_alpha), un signo de puntuación (.is_punct) o un dígito (. es_dígito) . Podemos usar directamente esta información para preprocesar nuestro corpus.\n",
    "\n",
    "La siguiente función implementa una canalización de normalización para un documento determinado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def normalize_Spacy(text):\n",
    "    text2 = nlp(text)\n",
    "    normalized_text = [w.lemma_.lower() for w in text2 if not w.is_stop \n",
    "                  and not w.is_punct and (w.is_alpha or w.is_digit)]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora normalizamos todo el corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_prec = []\n",
    "\n",
    "for fileid in movie_reviews.fileids():\n",
    "    text = movie_reviews.raw(fileid)\n",
    "    text_preproc = normalize_Spacy(text)\n",
    "    corpus_prec.append(text_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " 'teen',\n",
       " 'couple',\n",
       " 'church',\n",
       " 'party',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'accident',\n",
       " 'guy',\n",
       " 'die',\n",
       " 'girlfriend',\n",
       " 'continue',\n",
       " 'life',\n",
       " 'nightmare',\n",
       " 'deal',\n",
       " 'watch',\n",
       " 'movie',\n",
       " 'sorta',\n",
       " 'find',\n",
       " 'critique',\n",
       " 'mind',\n",
       " 'fuck',\n",
       " 'movie',\n",
       " 'teen',\n",
       " 'generation',\n",
       " 'touch',\n",
       " 'cool',\n",
       " 'idea',\n",
       " 'present',\n",
       " 'bad',\n",
       " 'package',\n",
       " 'make',\n",
       " 'review',\n",
       " 'hard',\n",
       " 'write',\n",
       " 'generally',\n",
       " 'applaud',\n",
       " 'film',\n",
       " 'attempt',\n",
       " 'break',\n",
       " 'mold',\n",
       " 'mess',\n",
       " 'head',\n",
       " 'lose',\n",
       " 'highway',\n",
       " 'memento',\n",
       " 'good',\n",
       " 'bad',\n",
       " 'way',\n",
       " 'make',\n",
       " 'type',\n",
       " 'film',\n",
       " 'folk',\n",
       " 'snag',\n",
       " 'correctly',\n",
       " 'take',\n",
       " 'pretty',\n",
       " 'neat',\n",
       " 'concept',\n",
       " 'execute',\n",
       " 'terribly',\n",
       " 'problem',\n",
       " 'movie',\n",
       " 'main',\n",
       " 'problem',\n",
       " 'simply',\n",
       " 'jumbled',\n",
       " 'start',\n",
       " 'normal',\n",
       " 'downshift',\n",
       " 'fantasy',\n",
       " 'world',\n",
       " 'audience',\n",
       " 'member',\n",
       " 'idea',\n",
       " 'go',\n",
       " 'dream',\n",
       " 'character',\n",
       " 'come',\n",
       " 'dead',\n",
       " 'look',\n",
       " 'like',\n",
       " 'dead',\n",
       " 'strange',\n",
       " 'apparition',\n",
       " 'disappearance',\n",
       " 'looooot',\n",
       " 'chase',\n",
       " 'scene',\n",
       " 'ton',\n",
       " 'weird',\n",
       " 'thing',\n",
       " 'happen',\n",
       " 'simply',\n",
       " 'explain',\n",
       " 'personally',\n",
       " 'mind',\n",
       " 'try',\n",
       " 'unravel',\n",
       " 'film',\n",
       " 'clue',\n",
       " 'kind',\n",
       " 'feed',\n",
       " 'film',\n",
       " 'big',\n",
       " 'problem',\n",
       " 'obviously',\n",
       " 'get',\n",
       " 'big',\n",
       " 'secret',\n",
       " 'hide',\n",
       " 'want',\n",
       " 'hide',\n",
       " 'completely',\n",
       " 'final',\n",
       " 'minute',\n",
       " 'thing',\n",
       " 'entertaining',\n",
       " 'thrilling',\n",
       " 'engage',\n",
       " 'meantime',\n",
       " 'sad',\n",
       " 'arrow',\n",
       " 'dig',\n",
       " 'flick',\n",
       " 'like',\n",
       " 'actually',\n",
       " 'figure',\n",
       " 'half',\n",
       " 'way',\n",
       " 'point',\n",
       " 'strangeness',\n",
       " 'start',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'sense',\n",
       " 'film',\n",
       " 'entertaining',\n",
       " 'guess',\n",
       " 'line',\n",
       " 'movie',\n",
       " 'like',\n",
       " 'sure',\n",
       " 'audience',\n",
       " 'give',\n",
       " 'secret',\n",
       " 'password',\n",
       " 'enter',\n",
       " 'world',\n",
       " 'understanding',\n",
       " 'mean',\n",
       " 'show',\n",
       " 'melissa',\n",
       " 'sagemiller',\n",
       " 'run',\n",
       " 'away',\n",
       " 'vision',\n",
       " '20',\n",
       " 'minute',\n",
       " 'movie',\n",
       " 'plain',\n",
       " 'lazy',\n",
       " 'okay',\n",
       " 'people',\n",
       " 'chase',\n",
       " 'know',\n",
       " 'need',\n",
       " 'give',\n",
       " 'different',\n",
       " 'scene',\n",
       " 'offer',\n",
       " 'insight',\n",
       " 'strangeness',\n",
       " 'go',\n",
       " 'movie',\n",
       " 'apparently',\n",
       " 'studio',\n",
       " 'take',\n",
       " 'film',\n",
       " 'away',\n",
       " 'director',\n",
       " 'chop',\n",
       " 'show',\n",
       " 'pretty',\n",
       " 'decent',\n",
       " 'teen',\n",
       " 'mind',\n",
       " 'fuck',\n",
       " 'movie',\n",
       " 'guess',\n",
       " 'suit',\n",
       " 'decide',\n",
       " 'turn',\n",
       " 'music',\n",
       " 'video',\n",
       " 'little',\n",
       " 'edge',\n",
       " 'sense',\n",
       " 'actor',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'wes',\n",
       " 'bentley',\n",
       " 'play',\n",
       " 'exact',\n",
       " 'character',\n",
       " 'american',\n",
       " 'beauty',\n",
       " 'new',\n",
       " 'neighborhood',\n",
       " 'big',\n",
       " 'kudo',\n",
       " 'sagemiller',\n",
       " 'hold',\n",
       " 'entire',\n",
       " 'film',\n",
       " 'actually',\n",
       " 'feel',\n",
       " 'character',\n",
       " 'unraveling',\n",
       " 'overall',\n",
       " 'film',\n",
       " 'stick',\n",
       " 'entertain',\n",
       " 'confusing',\n",
       " 'rarely',\n",
       " 'excite',\n",
       " 'feel',\n",
       " 'pretty',\n",
       " 'redundant',\n",
       " 'runtime',\n",
       " 'despite',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'ending',\n",
       " 'explanation',\n",
       " 'craziness',\n",
       " 'come',\n",
       " 'oh',\n",
       " 'way',\n",
       " 'horror',\n",
       " 'teen',\n",
       " 'slasher',\n",
       " 'flick',\n",
       " 'package',\n",
       " 'look',\n",
       " 'way',\n",
       " 'apparently',\n",
       " 'assume',\n",
       " 'genre',\n",
       " 'hot',\n",
       " 'kid',\n",
       " 'wrap',\n",
       " 'production',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'sit',\n",
       " 'shelf',\n",
       " 'skip',\n",
       " 'joblo',\n",
       " 'come',\n",
       " 'nightmare',\n",
       " 'elm',\n",
       " 'street',\n",
       " '3',\n",
       " 'blair',\n",
       " 'witch',\n",
       " '2',\n",
       " 'crow',\n",
       " 'crow',\n",
       " 'salvation',\n",
       " 'lose',\n",
       " 'highway',\n",
       " 'memento',\n",
       " 'stir',\n",
       " 'echo']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_prec[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contando las palabras más frecuentes en nuestro corpus\n",
    "\n",
    "Utilizando la clase `Counter` dentro de la librería `collections` podemos encontrar facilmente las palabras más utilizadas en nuestro corpus ya preprocesado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words_freq = Counter([w for d in corpus_prec for w in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = words_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 11171), ('movie', 6972), ('like', 3951), ('character', 3874), ('good', 3590), ('time', 2984), ('scene', 2670), ('story', 2343), ('play', 2324), ('come', 1969), ('know', 1965), ('bad', 1940), ('man', 1932), ('go', 1911), ('way', 1879), ('look', 1875), ('life', 1797), ('year', 1728), ('work', 1706), ('thing', 1661)]\n"
     ]
    }
   ],
   "source": [
    "print(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ejercicio**: Calcule los adjetivos más frecuentes en el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "QWvDrfYgN13e"
   ],
   "name": "Intro_NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
