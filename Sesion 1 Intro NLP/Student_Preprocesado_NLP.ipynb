{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "re9Lt_eGi1R-"
   },
   "source": [
    "# Procesamiento de Lenguaje Natural\n",
    "\n",
    "\n",
    "**Vanessa Gómez Verdejo, Emilio Parrado Hernández,  Pablo Martínez Olmos**\n",
    "\n",
    "Departamento de Teoría de la Señal y Comunicaciones\n",
    "\n",
    "**Universidad Carlos III de Madrid**\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZTTwuuIN0MH"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "# Figures plotted inside the notebook\n",
    "%config InlineBackend.figure_format = 'svg'  \n",
    "# High quality figures\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWvDrfYgN13e"
   },
   "source": [
    "# Introducción\n",
    "\n",
    "Hasta ahora hemos estado trabajando con datos de tipo  numérico o categórico. En esta sesión vamos a ver cómo trabajar con nuestros datos cuando éstos son cadenas de texto. A diferencia de los datos categóricos, en los que tenemos cadenas de texto asociadas a diferentes categorías y que podemos codificar fácilmente (por ejemplo, con un one-hot-encoding), cuando hablamos aquí de información textual nos referimos a frases, documentos y/o corpus de datos con estructura mucho más compleja. Idealmente, a partir de estos datos textuales tenemos que extraer la información necesaria (a poder ser incluyendo contenido semántico) y vectorizarla adecuadamente para poder utilizar o usar modelos de aprendizaje a partir de ella.\n",
    "\n",
    "En general, gran parte de la forma en que nos comunicamos hoy en día es a través de texto escrito, ya sea en servicios de mensajería, medios sociales y/o correo electrónico. Así, por ejemplo, en servicios/aplicaciones como TripAdvisor, Booking, Amazon, etc., los usuarios escriben reseñas de restaurantes/negocios, hoteles, productos para compartir sus opiniones sobre su experiencia. Estas reseñas, todas escritas en formato de texto, contienen una gran cantidad de información que sería útil responder preguntas relevantes para el negocio usando métodos de aprendizaje automático, por ejemplo, para predecir el mejor restaurante en una determinada zona. \n",
    "\n",
    "Este tipo de tarea (preprocesado) se denomina **procesamiento del lenguaje natural** (Natural Language Processing, NLP).\n",
    "El NLP es un subcampo de la lingüística, la informática y la inteligencia artificial que se ocupa de las interacciones entre los ordenadores (o procesadores) y el lenguaje humano; en particular engloba un conjunto de técnicas para permitir que los ordenadores procesen y analicen grandes cantidades de texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3v4bBAVOM2z"
   },
   "source": [
    "# Pipeline para el procesado de texto \n",
    "\n",
    "Como sabemos, los algoritmos de ML procesan números, no palabras, por lo que necesitamos transformar el texto en números significativos que contengan la información relevante de los documentos. Este proceso de convesión de texto a números es lo que llamaremos **vectorización**. \n",
    "\n",
    "No obstante, para tener una representación útil, se requieren normalmente algunos pasos de **preprocesamiento** previo que limpien y homogenizen los documentos: tokenización, eliminación de *stop-words*, lematización, etc.\n",
    "La siguiente figura muestra los diferentes pasos que debemos seguir para procesar nuestros documentos hasta poder ser utilizados por nuestro modelo de aprendizaje:\n",
    "\n",
    "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/BBVA/NLP/PipelineNLP.png\" width=\"80%\"> \n",
    "\n",
    "A lo largo de este notebook, veremos las herramientas que tenemos disponibles en Python para llevar a cabo todos estos pasos. Concretamente, nos centraremos en el uso de dos librerias:\n",
    "* [NLTK, Natural Language ToolKit](https://www.nltk.org/). Esta libreria es una excelente biblioteca de NLP escrita en Python por expertos tanto del mundo académico como de la industria. NLTK Permite crear aplicaciones con datos textuales rápidamente, ya que proporciona un conjunto de clases básicas para trabajar con corpus de datos, incluyendo colecciones de textos (corpus), listas de palabras clave, clases para representar y operar con datos de tipo texto (documentos, frases, palabras, ...) y funciones para realizar tareas comunes de NLP (conversión a token, conteo de palabras, ...). Por lo que va a ser de gran ayuda para el preprocesado de los documentos.\n",
    "* [Gensim](https://pypi.org/project/gensim/) es otra librería de Python para el modelado por temáticas (*topic modeling*), la indexación de documentos y tareas de recuperación de la información para documentos. Está diseñada para operar con grandes cantidades de información (con implementaciones eficientes y paralelizables/distribuidas) y nos va a ser de gran ayuda para la vectorización de nuestros corpus de datos una vez preprocesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X22h_atA04h5"
   },
   "source": [
    "Empecemos este notebook cargando la librería NLTK y algunas de sus funcionalidades. A continuación, elegiremos un corpus de datos con el que empezar a analizar las funcionalidades básicas que aportan NLTK y Gensim y sobre el que veremos, uno por uno, en qué consisten los diferentes pasos de nuestro pipeline y cómo podemos implementarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zycI8RTyKLaR",
    "outputId": "85a2211c-dfae-4856-b61c-e2cd93f33bb8"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8JGdmgIKLaW"
   },
   "source": [
    "# 1. Cargando nuestro corpus de datos\n",
    "\n",
    "NLTK incluye diferentes corpus de datos  con los que podemos probar nuestras herramientas. Podemos encontrar información de todos ellos en [NLTK corpus](https://www.nltk.org/book/ch02.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFk2G2d36yzm"
   },
   "source": [
    "## El objeto CorpusReader\n",
    "Para empezar a trabajar vamos a usar el corpus **inaugural**, uno de los corpus de datos incluidos en NLTK y que consiste en 58 documentos de texto con los discursos presidenciales de los presidentes de EEUU.\n",
    "\n",
    "La siguiente celda de código nos muestra cómo cargar el corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5qadzt_ee6Q",
    "outputId": "07ff7084-02d7-43bf-d166-b4233c798ba4"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "nltk.download('inaugural')\n",
    "inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3Ze2_iK4dY7"
   },
   "source": [
    "Al cargar el corpus, se genera un objeto de tipo `CorpusReader`, llamado `inaugural`, con el contenido del mismo. Dado que un corpus es una colección de documentos/textos, podemos ver que documentos componen este corpus usando la función `.fileids()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7sqiCPK4eFM",
    "outputId": "572ae106-b21b-4101-fd87-05a22e392c43"
   },
   "outputs": [],
   "source": [
    "print(inaugural.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JbA4shyedVq"
   },
   "source": [
    "Una característica de este corpus es que los documentos que lo forman tienen información sobre el año de cada documento. Podemos crear una lista de los años de cada discurso usando [list comprenhension](https://www.pythonforbeginners.com/basics/list-comprehensions-in-python):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7gvntUQmhotH",
    "outputId": "32110ad3-f78c-4f97-cf4e-a930065e64d7"
   },
   "outputs": [],
   "source": [
    "years = [fileid[:4] for fileid in inaugural.fileids()]\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5cEbc1o6GGn"
   },
   "source": [
    "También podemos usar la función `.words()` de `inaugural` para acceder a las palabras que componen los documentos del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "duI8edHeh7hd",
    "outputId": "7c6086a5-fffd-4688-ecdc-876bb3901238"
   },
   "outputs": [],
   "source": [
    "print(inaugural.words())\n",
    "print(len(inaugural.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tEPerLiKLaX"
   },
   "source": [
    "Si queremos podemos acceder a un **documento** concreto del corpus y extraer su contenido en crudo con la función `.raw()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "oXSjzPneeiML",
    "outputId": "72c148db-f73d-4028-c060-c8c202b45a4b"
   },
   "outputs": [],
   "source": [
    "trump_text = inaugural.raw('2017-Trump.txt')\n",
    "trump_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIHDWtJnKLaY"
   },
   "source": [
    "La varaible `trump_text` que obtenemos es un único string (con todas las funciones de los tipos string) que contiene todas las palabras del documento que hemos especificado. Por ejemplo, podemos ver los primeros 20 caracteres de este documento como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhH8ijPXKLaY",
    "outputId": "02b0a43d-736e-4ee8-fef6-6f3d4c1f603d"
   },
   "outputs": [],
   "source": [
    "print(type(trump_text))\n",
    "\n",
    "print(trump_text[:20])\n",
    "\n",
    "print('\\n The total number of characters in the document is %d' %(len(trump_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSOK0XTDKLaa"
   },
   "source": [
    "El CorpusReader también nos permite cargar documentos estructurados por **frases**. Para ello tenemos que usar la función `.sents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFUZuuBbfdHm",
    "outputId": "2d8a7336-4007-4db0-e779-af80afdf4602"
   },
   "outputs": [],
   "source": [
    "trump_sents = inaugural.sents('2017-Trump.txt')\n",
    "print('\\n The total number of sentences in the document is %d' %(len(trump_sents)))\n",
    "print(trump_sents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG52tAxTiH-A"
   },
   "source": [
    "O, directamente, cargarlo a nivel de **palabras** (o **tokens**) usando el método `.words()`. Nótese que cuando hablamos de palabra o token no sólo son palabras con significado, sino que pueden ser números o signos de puntuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eumGbIWLiJRK",
    "outputId": "b6223e01-92a2-46aa-8f71-2f02ec689ccd"
   },
   "outputs": [],
   "source": [
    "trump_words = inaugural.words('2017-Trump.txt')\n",
    "print('\\n The total number of words in the document is %d' %(len(trump_words)))\n",
    "print(trump_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xojODsSNb4G"
   },
   "source": [
    "# 2. Preprocesado del corpus\n",
    "\n",
    "Antes de transformar los datos de entrada de texto en una representación vectorial, necesitamos estructurar y limpiar el texto, y conservar toda la información que permita capturar el contenido semántico del corpus.\n",
    "\n",
    "Para ello, el procesado típico de NLP aplica los siguientes pasos:\n",
    "\n",
    "1. Tokenización\n",
    "2. Homogeneización\n",
    "3. Limpieza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBOayTbJNb4G"
   },
   "source": [
    "## 2.1. Tokenization\n",
    "\n",
    "**Tokenización** es el proceso de dividir el texto dado en piezas más pequeñas llamadas tokens. Las palabras, los números, los signos de puntuación y otros pueden ser considerados como tokens.\n",
    "\n",
    "Ya hemos visto que el objeto `CorpusReader` incluye funciones para dividir el corpus en frases o palabras. Pero NLTK incluye también funciones genéricas para hacer estas operaciones sobre cualquier cadena de texto. En concreto, tiene dos funciones:\n",
    "- `sent_tokenize`: es un tokenizador de frases. Este tokenizador divide un texto en una lista de oraciones. Para decidir dónde empieza o acaba una frase NLTK tiene un modelo pre-entrenado para el idioma específico en el que estemos trabajando. Este modelo lo hemos cargado al principio con `nltk.download('punkt')`.\n",
    "- `word_tokenize`/`wordpunct_tokenize`:  Divide un texto en palabras u otros caracteres individuales cómo pueden ser signos de puntuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbCGdSySgtXy"
   },
   "source": [
    "Veamos como funcionan estas funciones con el siguiente texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W73SiY9Wgwkh"
   },
   "outputs": [],
   "source": [
    "texto = 'I\\'m a dog and it\\'s great! You\\'re cool and Sandy\\'s book is big. Don\\'t tell her, you\\'ll regret it! \"Hey\", she\\'ll say!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-NYRnSP1g4wN",
    "outputId": "d3dda7f4-ff6a-47c1-e033-fe73ee2f3651"
   },
   "outputs": [],
   "source": [
    "sent=nltk.sent_tokenize(texto)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDA-wzX9sGVc",
    "outputId": "b716aa85-3c26-42f7-ca00-941c48b7a0db"
   },
   "outputs": [],
   "source": [
    "sent_tokens1=nltk.wordpunct_tokenize(texto)\n",
    "print(sent_tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rp6mC8AtsOUP",
    "outputId": "5dded205-6b66-478e-accb-6fab560e754d"
   },
   "outputs": [],
   "source": [
    "sent_tokens2=nltk.word_tokenize(texto)\n",
    "print(sent_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW6OyNgfiwT_"
   },
   "source": [
    "Aunque puede parecer que las funciones `wordpunct_tokenize` y `word_tokenize` hacen lo mismo, con este ejemplo vemos que `wordpunct_tokenize` permite separar los signos de puntuación mientras que `word_tokenize` no.  Nótese la diferencia al dividir `I'm` entre ambas funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE4YjZJFrsEk"
   },
   "source": [
    "También podemos combinar `sent_tokenize` y `word_tokenize` para tener frases y cada frase divida en tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PU96hUdSKLao",
    "outputId": "5234ae34-e142-462a-b8ac-ecd07485f589"
   },
   "outputs": [],
   "source": [
    "for sent in nltk.sent_tokenize(texto):\n",
    "    print(nltk.wordpunct_tokenize(sent))\n",
    "    print(\"**************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPr7lOOQjYJe"
   },
   "source": [
    "#### Ejercicio 1: Tokenización del texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uckiOuc0fL_l"
   },
   "source": [
    "Seleccionemos ahora uno de los textos de nuestro corpus y veamos cómo aplicar estas funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YN3ElSgsKLao"
   },
   "outputs": [],
   "source": [
    "# Get a text\n",
    "trump_text = inaugural.raw('2017-Trump.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yy08XX8Nl-wS"
   },
   "source": [
    "Complete la siguiente celda de código para dividir el texto en frases e imprima las 5 primeras frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pO6aEgfTjjDM",
    "outputId": "68089697-1fc6-4df2-9298-8f6f2acd9d40"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IGfAg7pmXgc"
   },
   "source": [
    "Complete las siguientes celdas de código para dividir el texto en tokens (considerando y sin considerar la puntuación) e imprima los primeros 5 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJv-YzJ8pMih",
    "outputId": "58859023-775e-4177-a569-41ccb432383d"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HK8nwtcMfztY",
    "outputId": "e931d3fb-7b23-489d-88f3-4b4ab46a31a1"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8T-OErnWNb4H"
   },
   "source": [
    "## 2.2. Homogeneización\n",
    "\n",
    "Al observar los tokens del corpus podemos ver que hay muchos tokens con algunas letras en mayúsculas y otras en minúsculas, el mismo token unas veces aparece en singular y otras en plural, o el mismo verbo que aparece en diferentes tiempos verbales. Para analizar semánticamente el texto, nos interesa  **homogeneizar** las palabras que formalmente son diferentes pero tienen el mismo significado.\n",
    "\n",
    "Para ello podemos usar las herramientas de lematización de NLTK. El proceso habitual de homogeneización consiste en los siguientes pasos:\n",
    "\n",
    "1. Eliminación de las mayúsculas y caracteres no alfanuméricos: de este modo los caracteres alfabéticos en mayúsculas se transformarán en sus correspondientes caracteres en minúsculas y  se eliminarán los caracteres no alfanuméricos, por ejemplo, los signos de puntuación.\n",
    "\n",
    "2. Stemming/Lematización: eliminar las terminaciones de las palabras para preservar su raíz de las palabras e ignorar la información gramatical (eliminamos marcas de plurales, género, conjugaciones verbales, ...).\n",
    "\n",
    "Veamos como ir aplicando uno a uno cada uno de estos pasos sobre el texto anterior una vez tokenizado por palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "leDLNxMBoeRH",
    "outputId": "312e3fde-700a-476e-d9f7-e089206e281c"
   },
   "outputs": [],
   "source": [
    "# Get and tokenize the text\n",
    "trump_text = inaugural.raw('2017-Trump.txt')\n",
    "trump_tokens=nltk.wordpunct_tokenize(trump_text)\n",
    "print(trump_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SuawzwINb4H"
   },
   "source": [
    "#### **Ejercicio 2**: eliminación de mayúsculas y caracteres no alfanuméricos\n",
    "\n",
    "Convierte todos los tokens de `trump_tokens` a minúsculas (usando el método `.lower()`) y elimina los tokens no alfanuméricos (que puedes detectar con el método `.isalnum()`). Este procesado puedes codificarlo una sola línea de código usando list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5d22gbQuK7P",
    "outputId": "046f99d9-3375-40a7-8bdb-0318468a16e5"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6ozm2RjKLap"
   },
   "source": [
    "**Stemming and Lemmatización**\n",
    "\n",
    "En el lenguaje común, las palabras pueden tomar diferentes formas indicando género, cantidad, tiempo (en el caso de los verbos), formas concretas para nombres/adjetivos o adverbios, ... Para muchas aplicaciones, es útil normalizar estas formas en alguna palabra canónica que facilite su análisis. Hay dos maneras de realizar este proceso:\n",
    "\n",
    "1. El proceso de **stemming** reduce las palabras a su base o raíz \n",
    "\n",
    "      running --> run\n",
    "\n",
    "      flowers --> flower\n",
    "\n",
    "  Para poder hacer esta transformación necesitamos librerías específicas que tienen almacenadas para el vocabulario de cada idioma las raices de dicho vocabulario y hacen esta conversión. En NLTK hay varios stemmers disponibles:\n",
    "  * Lancaster (inglés, es más reciente y bastante agresivo)\n",
    "  * Porter (inglés, es el stemmer original)\n",
    "  * Snowball  (incluye muchos idiomas y es el más nuevo)\n",
    "\n",
    "    \n",
    "2. El objetivo de la **lematización**, al igual que el stemmer, es reducir las formas inflexionales a una forma base común. A diferencia del steming, la lematización no se limita a cortar las inflexiones. En su lugar, utiliza bases de conocimiento léxico para obtener las formas básicas correctas de las palabras. \n",
    "\n",
    "    women   --> woman\n",
    "\n",
    "    foxes   --> fox\n",
    "    \n",
    "  La lematización en NLTK se basa en el léxico de [WordNet](https://wordnet.princeton.edu/). WordNet es un diccionario de inglés de orientación semántica, incluye el inglés WordNet con 155.287 palabras y 117.659 conjuntos de sinónimos. \n",
    "\n",
    "Veamos cómo funcionan el stemming y la lematización con un ejemplo:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKO2XsUjKLap",
    "outputId": "b6025a43-1f92-4527-e965-00562895e7fc"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "text = list(nltk.word_tokenize(\"The women running in the fog passed bunnies working as computer scientists.\"))\n",
    "\n",
    "# Load several stemmers \n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for stemmer in (snowball, lancaster, porter):\n",
    "    stemmed_text = [stemmer.stem(t) for t in text]\n",
    "    print(\" \".join(stemmed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTfrNKpWKLaq",
    "outputId": "261b6fb8-4ffc-4423-9c96-9f78b5c87815"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Try lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "print(\" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNQXBpS5nah7"
   },
   "source": [
    "Compara cómo los diferentes procesos transforman palabras como `women`, `running` o `computer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPcY6x5HsjzP"
   },
   "source": [
    "Una de las ventajas de la lematización es que el resultado sigue siendo una palabra, lo que es más aconsejable para la presentación de los resultados del procesado de textos.\n",
    "\n",
    "Sin embargo, sin utilizar información contextual, `lemmatize()` no elimina las diferencias gramaticales. Por esta razón, \"running\" o \"passed\" se conservan y no se sustituyen por el infinitivo \"run\" o \"pass\".\n",
    "\n",
    "Como alternativa, podemos aplicar `.lemmatize(word, pos)`, donde `pos` es un código de cadena que especifica función gramatical de las palabras en su oración. Por ejemplo, se puede comprobar la diferencia entre `wnl.lemmatize('running')` y `wnl.lemmatize('running', pos='v')`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aVD_Ys1s9ru",
    "outputId": "50835579-28ee-4cc9-9f18-1e2ccf392be0"
   },
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize('running'))\n",
    "print(lemmatizer.lemmatize('running', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKaflPOf-4N-"
   },
   "source": [
    "Notese que ninguno de los dos da una solución perfecta... al final este proceso requiere de supervisión manual que nos permita afinar esta homogeneización lo mejor posible. En ocasiones lo que se hace es que se aplica un primer lematizado y sobre un stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EXCRMlFbEBVZ",
    "outputId": "f24773a1-b3ca-42f5-83d5-5e975153839f"
   },
   "outputs": [],
   "source": [
    "lemmas_text = [lemmatizer.lemmatize(t) for t in text]\n",
    "stemmed_text = [snowball.stem(t) for t in lemmas_text]\n",
    "\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v90trBdQ9cyY"
   },
   "source": [
    "#### **Ejercicio 3**: Stemming/Lematización\n",
    "\n",
    "Aplique el proceso de stemming y lematización sobre el texto del discurso inaugural de Trump resultantes del proceso de filtrado anterior (salida del Ejercicio 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ms6f0RXL-a4A",
    "outputId": "bac29157-ef19-497b-84fc-08c756ae2008"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCd3LMgCH8Ra"
   },
   "source": [
    "## 2.3. Limpieza\n",
    "\n",
    "El último paso del preprocesado consiste en eliminar las palabras irrelevantes o **stop words** de los documentos. Las **stop words** son las palabras más comunes en un idioma como \"el\", \"a\", \"sobre\", \"es\", \"todo\". Estas palabras no tienen un significado importante y normalmente se eliminan de los textos. Para aplicar este proceso, se cargan librerías específicas que contienen este listado de palabras por cada idioma.\n",
    "\n",
    "Además, este paso suele utilizarse para eliminar las marcas de puntaución (',', '.', '?', ....), para lo que también tenemos que cargar otro módulo con los signos de puntuación. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdCAXCR9ALKJ",
    "outputId": "74d057fa-3997-448c-b426-57852979f214"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "print(stopwords_en[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDPi-iMFqFVF",
    "outputId": "6a84c4e0-fbfb-40a6-b293-c5b0d72d6f39"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = string.punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4qC3yWvp1mo"
   },
   "source": [
    "\n",
    "Veamos como aplicarlo con un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_W-y1Usvp2xa",
    "outputId": "2c0b72ff-bd35-4ea6-eba0-988aa3e66f41"
   },
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize('I\\'m a dog and it\\'s great! You\\'re cool and Sandy\\'s book is big. Don\\'t tell her, you\\'ll regret it! \"Hey\", she\\'ll say!')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsnBEDfEqeCj",
    "outputId": "fa7dbae9-c961-40d1-db34-bb3727756dcd"
   },
   "outputs": [],
   "source": [
    "clean_text1 = [token for token in text if (token not in stopwords_en)] \n",
    "print(clean_text1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSxNb9HRqdbO",
    "outputId": "7d0b7733-9f98-4719-d947-e1eb10fdc11b"
   },
   "outputs": [],
   "source": [
    "clean_text2 = [token for token in text if (token not in punctuation)] \n",
    "print(clean_text2)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jyiGiS7Ajuf"
   },
   "source": [
    "#### **Ejercicio 4**: Eliminación de stop-words y puntuación\n",
    "\n",
    "Aplique conjuntamente el proceso de eliminación de stop-words y puntuación sobre el texto del discurso inaugural de Trump resultantes del proceso de filtrado y lematización anterior (salida Ejercicio 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5_QYLhQ3BYOK",
    "outputId": "d3f21253-a6d7-4a43-f95c-c9474d6cb55c"
   },
   "outputs": [],
   "source": [
    "print('Texto filtrado y lematizado:')\n",
    "print(trump_tokens_stem)\n",
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIMy4KgqHFut"
   },
   "source": [
    "Nota: En el primer paso ya eliminamos las marcas de puntuación con la función es `.isalnum()`; por lo que no sería necesario eliminar de nuevo la puntuación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKx8y63AAis6"
   },
   "source": [
    "## 2.4 Pipeline de preprocesado o normalización del texto\n",
    "\n",
    "Por último, y para facilitar su uso posterior, vamos a juntar estos tres pasos en una única función que nos permita realizar la tokenización, homogeneización y limpieza con una única llamada a una función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tG8F3-2Frhzq"
   },
   "source": [
    "#### **Ejercicio 5**: Función para la normalización de textos\n",
    "\n",
    "Complete el código de la siguiente función para poder hacer todos los pasos anteriores en una única función y luego pruebe a utilizarla sobre el texto del discurso inaugural de Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6BlL_PAKLaq"
   },
   "outputs": [],
   "source": [
    "## Load Modules\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "snowball = SnowballStemmer('english')\n",
    "stopwords   = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = []\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        #<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_text = inaugural.raw('2017-Trump.txt')\n",
    "print('Texto original (primeros 200 caracteres...):')\n",
    "print(trump_text[:200])\n",
    "trump_text_preproc = normalize(trump_text)\n",
    "print('*******************')\n",
    "print('Texto preprocesado:')\n",
    "print(trump_text_preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 6**: Preprocesando el corpus de datos\n",
    "\n",
    "Para poder trabajar de ahora en adelante con todos los documentos del corpus de datos preprocesados, aplique el pipeline de preprocesado a todos los documentos del corpus de datos `inaugural`. Guarde el resultado en una variable de tipo lista, llamada `corpus_prec`, donde cada elemento de la lista será un texto preprocesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3N0vvTVNIKf",
    "outputId": "0539a2da-d8c0-47c7-ac55-38a88d10cd3d"
   },
   "outputs": [],
   "source": [
    "print('Número de documentos en el corpus preprocesado:')\n",
    "print(len(corpus_prec))\n",
    "print('**********')\n",
    "print('Algunos de los elementos del primer documento preprocesado')\n",
    "print(corpus_prec[0][:20])\n",
    "print('**********')\n",
    "print('Algunos de los elementos del segundo documento preprocesado')\n",
    "print(corpus_prec[1][:20])\n",
    "print('**********')\n",
    "print('Algunos de los elementos del tercer documento preprocesado')\n",
    "print(corpus_prec[3][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUmd8ms2Nb4J"
   },
   "source": [
    "# 3. Vectorización\n",
    "\n",
    "Hasta este punto, hemos transformado la colección de textos en bruto en una lista de textos, en la que cada texto es una colección de las raíces de las palabras más relevantes para el análisis semántico. Ahora, necesitamos convertir estos datos (una lista de listas de tokens) en una representación numérica (una lista de vectores, o una matriz). \n",
    "\n",
    "Antes de pasar a hacer esta vectorización, documento a documento, vamos a hacer un **análisis frecuencial** del contenido del corpus preprocesado. Para ello vamos a obtener:\n",
    "- Un recuento de palabras: número de veces que aparece cada palabra en el corpus.\n",
    "- El vocabulario: conjunto de palabras únicas dentro del corpus.\n",
    "- La diversidad léxica: la relación entre el número de palabras y el vocabulario.\n",
    "\n",
    "Y para ello vamos a usar la dos clases muy útiles de NLTK que nos permiten hacer estos análisis de frecuencia:\n",
    "\n",
    "- `FreqDist`\n",
    "- `ConditionalFreqDist` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dhfTeTtKLai"
   },
   "source": [
    "## 3.1 Análisis de frecuencias del corpus\n",
    "\n",
    "Para empezar a hacer este análisis, vamos a convertir nuestra lista de documentos, donde cada documento tiene una lista de tokens, en una única lista con todos los tokens del corpus. Una vez hecha esta conversión, utilizamos `FreqDist` y algunos de sus métodos para analizar frecuencialmente el contenido del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HB_PeBSnKLai"
   },
   "outputs": [],
   "source": [
    "tokens_corpus = [token for doc in corpus_prec for token in doc]\n",
    "counts  = nltk.FreqDist(tokens_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tQQr-aSKLai"
   },
   "source": [
    "Podemos comprobar que `counts` es un diccionario que contiene el número de veces que cada palabra aparece en el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BEF4pjjLKLai",
    "outputId": "9e26af6b-9463-4cac-a310-8b2646822840"
   },
   "outputs": [],
   "source": [
    "counts # counts is a FreqDist object, a dictionary data type with additional methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ou-1tNAyKLai",
    "outputId": "5081b9ca-6c9b-45e7-c3d4-df32a2328d64"
   },
   "outputs": [],
   "source": [
    "counts['citizen']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThfnxpjlKLai"
   },
   "source": [
    "Simplemente operando sobre `counts` podemos calcular fácilmente el tamaño del vocabulario y la diversidad léxica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXNqNSXTKLaj",
    "outputId": "1fc11f0e-edd5-42c3-9b26-f88049fc885f"
   },
   "outputs": [],
   "source": [
    "vocab   = len(counts.keys()) \n",
    "words   = sum(counts.values())\n",
    "lexdiv  = float(words) / float(vocab)\n",
    "\n",
    "print(\"El corpus tiene %i palabras únicas y un total de %i palabras con una diversidad léxica de %0.3f\" % (vocab, words, lexdiv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRAg-kG4KLak"
   },
   "source": [
    "Con el método  `most_common(n)` obtenemos una lista de las palabras más comunes. Haciendo `counts.plot(n,cumulative=True)` podemos tener una idea de cuánto dominan las palabras más comunes en el corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lqFOCZ1KLak",
    "outputId": "33680450-c9b9-4134-dda4-a364b186babe"
   },
   "outputs": [],
   "source": [
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "Ez_aDeoZKLak",
    "outputId": "95c8f793-79c9-4ac7-a862-dbcbcd15ec2f"
   },
   "outputs": [],
   "source": [
    "counts.plot(40, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6Bn4vDgL7IL"
   },
   "source": [
    "Otra forma de analizar la frecuencia de aparición de las palabras en el corpus es dibujando una nube llena de muchas palabras de diferentes tamaños, que representan la frecuencia o la importancia de cada palabra. Esto es lo que se conoce como **word cloud** o **nube de palabras**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "L7LetRlNKeMb",
    "outputId": "4e006466-d62a-4f81-a604-f1d176523f78"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=40, background_color=\"white\").generate(' '.join(tokens_corpus))\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewKUSJZK11fh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "QWvDrfYgN13e"
   ],
   "name": "Intro_NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
