{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de Lenguaje Natural\n",
    "\n",
    "**Pablo Martínez Olmos**\n",
    "\n",
    "Departamento de Teoría de la Señal y Comunicaciones\n",
    "\n",
    "**Universidad Carlos III de Madrid**\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA) para modelado de tópicos\n",
    "\n",
    "En este notebook vamos implementar LDA sobre el dataset [NeurIPS papers](https://www.kaggle.com/rowhitswami/nips-papers-1987-2019-updated?select=papers.csv). Este dataset contiene los títulos, abstract y texto de los artículos publicados en la conferencia [NeurIPS (antes NIPS)](https://nips.cc/), la internacional conferencia más importante en aspecto relacionados con aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "# Figures plotted inside the notebook\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "# High quality figures\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers =  pd.read_csv(\"http://www.tsc.uc3m.es/~olmos/BBVA/papers.csv\")\n",
    "\n",
    "#papers =  pd.read_csv(\"papers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos vamos a quedar con los artículos más recientes, a partir de 2010 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_nips = papers.loc[papers['year']>=2010].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3853</th>\n",
       "      <td>363</td>\n",
       "      <td>2010</td>\n",
       "      <td>Agnostic Active Learning Without Constraints</td>\n",
       "      <td>We present and analyze an agnostic active lear...</td>\n",
       "      <td>Agnostic Active Learning Without Constraints\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3854</th>\n",
       "      <td>1164</td>\n",
       "      <td>2010</td>\n",
       "      <td>A Dirty Model for Multi-task Learning</td>\n",
       "      <td>We consider the multiple linear regression pro...</td>\n",
       "      <td>A Dirty Model for Multi-task Learning\\n\\nAli J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3855</th>\n",
       "      <td>348</td>\n",
       "      <td>2010</td>\n",
       "      <td>Generative Local Metric Learning for Nearest N...</td>\n",
       "      <td>We consider the problem of learning a local me...</td>\n",
       "      <td>Generative Local Metric Learning for\\n\\nNeares...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3856</th>\n",
       "      <td>899</td>\n",
       "      <td>2010</td>\n",
       "      <td>Relaxed Clipping: A Global Training Method for...</td>\n",
       "      <td>Robust regression and classification are often...</td>\n",
       "      <td>Relaxed Clipping: A Global Training Method\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3857</th>\n",
       "      <td>1195</td>\n",
       "      <td>2010</td>\n",
       "      <td>Linear readout from a neural population with p...</td>\n",
       "      <td>How much information does a neural population ...</td>\n",
       "      <td>Linear readout from a neural population\\n\\nwit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9675</th>\n",
       "      <td>5452</td>\n",
       "      <td>2019</td>\n",
       "      <td>Discrete Object Generation with Reversible Ind...</td>\n",
       "      <td>The success of generative modeling in continuo...</td>\n",
       "      <td>Discrete Object Generation\\n\\nwith Reversible ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9676</th>\n",
       "      <td>4799</td>\n",
       "      <td>2019</td>\n",
       "      <td>Adaptively Aligned Image Captioning via Adapti...</td>\n",
       "      <td>Recent neural models for image captioning usua...</td>\n",
       "      <td>Adaptively Aligned Image Captioning via\\n\\nAda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9677</th>\n",
       "      <td>1827</td>\n",
       "      <td>2019</td>\n",
       "      <td>Fully Dynamic Consistent Facility Location</td>\n",
       "      <td>We consider classic clustering problems in ful...</td>\n",
       "      <td>Fully Dynamic Consistent Facility Location\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9678</th>\n",
       "      <td>8693</td>\n",
       "      <td>2019</td>\n",
       "      <td>Efficient Rematerialization for Deep Networks</td>\n",
       "      <td>When training complex neural networks, memory ...</td>\n",
       "      <td>Efﬁcient Rematerialization for Deep Networks\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9679</th>\n",
       "      <td>2302</td>\n",
       "      <td>2019</td>\n",
       "      <td>Flow-based Image-to-Image Translation with Fea...</td>\n",
       "      <td>Learning non-deterministic dynamics and intrin...</td>\n",
       "      <td>Flow-based Image-to-Image Translation\\n\\nwith ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5827 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      source_id  year                                              title  \\\n",
       "3853        363  2010       Agnostic Active Learning Without Constraints   \n",
       "3854       1164  2010              A Dirty Model for Multi-task Learning   \n",
       "3855        348  2010  Generative Local Metric Learning for Nearest N...   \n",
       "3856        899  2010  Relaxed Clipping: A Global Training Method for...   \n",
       "3857       1195  2010  Linear readout from a neural population with p...   \n",
       "...         ...   ...                                                ...   \n",
       "9675       5452  2019  Discrete Object Generation with Reversible Ind...   \n",
       "9676       4799  2019  Adaptively Aligned Image Captioning via Adapti...   \n",
       "9677       1827  2019         Fully Dynamic Consistent Facility Location   \n",
       "9678       8693  2019      Efficient Rematerialization for Deep Networks   \n",
       "9679       2302  2019  Flow-based Image-to-Image Translation with Fea...   \n",
       "\n",
       "                                               abstract  \\\n",
       "3853  We present and analyze an agnostic active lear...   \n",
       "3854  We consider the multiple linear regression pro...   \n",
       "3855  We consider the problem of learning a local me...   \n",
       "3856  Robust regression and classification are often...   \n",
       "3857  How much information does a neural population ...   \n",
       "...                                                 ...   \n",
       "9675  The success of generative modeling in continuo...   \n",
       "9676  Recent neural models for image captioning usua...   \n",
       "9677  We consider classic clustering problems in ful...   \n",
       "9678  When training complex neural networks, memory ...   \n",
       "9679  Learning non-deterministic dynamics and intrin...   \n",
       "\n",
       "                                              full_text  \n",
       "3853  Agnostic Active Learning Without Constraints\\n...  \n",
       "3854  A Dirty Model for Multi-task Learning\\n\\nAli J...  \n",
       "3855  Generative Local Metric Learning for\\n\\nNeares...  \n",
       "3856  Relaxed Clipping: A Global Training Method\\n\\n...  \n",
       "3857  Linear readout from a neural population\\n\\nwit...  \n",
       "...                                                 ...  \n",
       "9675  Discrete Object Generation\\n\\nwith Reversible ...  \n",
       "9676  Adaptively Aligned Image Captioning via\\n\\nAda...  \n",
       "9677  Fully Dynamic Consistent Facility Location\\n\\n...  \n",
       "9678  Efﬁcient Rematerialization for Deep Networks\\n...  \n",
       "9679  Flow-based Image-to-Image Translation\\n\\nwith ...  \n",
       "\n",
       "[5827 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recent_nips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, vamos a concatenar título y abstract para hacer LDA con el campo resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_nips['Title_abstract'] = recent_nips['title'].map(str) + ' ' + recent_nips['abstract'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a normalizar el dataset, elimnando stopping words y signos de puntuación ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def normalize(doc):\n",
    "    \n",
    "    return [w.lemma_.lower() for w in doc if not w.is_stop and not w.is_punct and w.is_alpha] # Solo tokens alfabéticos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_norm = [normalize(nlp(s)) for s in recent_nips['Title_abstract']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para validar el número de tópicos, vamos a dividir en entrenamiento y validación. Para ello, hacemos un random  shuffle (originalmente los artículos están ordenados por fecha) y partimos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(papers_norm )\n",
    "               \n",
    "papers_norm_train = papers_norm[:5000]\n",
    "\n",
    "papers_norm_val = papers_norm[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el diccionario y lo filtramos, importante para obtener topicos interpretables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "D = gensim.corpora.Dictionary(papers_norm_train)\n",
    "\n",
    "n_tokens = len(D)\n",
    "\n",
    "no_below = 10 #Minimum number of documents to keep a term in the dictionary\n",
    "no_above = .30 #Maximum proportion of documents in which a term can appear to be kept in the dictionary\n",
    "\n",
    "D.filter_extremes(no_below=no_below,no_above=no_above)\n",
    "\n",
    "n_tokens = len(D)\n",
    "\n",
    "print('The dictionary contains', n_tokens, 'terms')\n",
    "print('First terms in the dictionary:')\n",
    "for n in range(10):\n",
    "    print(str(n), ':', D[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la matriz BOW ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "corpus_bow = [D.doc2bow(doc) for doc in papers_norm_train]\n",
    "\n",
    "corpus_bow_val = [D.doc2bow(doc) for doc in papers_norm_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "[**LDA es un modelo generativo probabilístico para textos**](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf). En particular, asumiendo un número $K$ de temas, LDA plantea el siguiente proceso generativo:\n",
    "\n",
    "<kbd>\n",
    "<img src='http://www.tsc.uc3m.es/~olmos/BBVA/LDA_modelo.png' width=600 />\n",
    "</kbd>\n",
    "\n",
    "El proceso anterior, matemáticamente se define como un **modelo de mezcla de variables categóricas** (en lugar de Gaussianas como en un GMM). La siguiente figura ([Fuente](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)), resume el proceso anterior:\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~olmos/BBVA/LDA1.png' width=800 />\n",
    "\n",
    "Es importante notar que $\\theta_d$, la proporción de temas en cada documento es una **representación vectorial del mismo que resume su información**. Este vector puede ser utilizado como vector de características para realizar otros algoritmos de aprendizaje (búsqueda de vecinos más cercanos, clasificación de documentos, etc ...).\n",
    "\n",
    "El objetivo en el entrenamiento LDA es, **dada una colección de documentos representados como un BoW** $\\mathbf{X}= [\\mathbf{x}_1,\\ldots,\\mathbf{x}_N]$, obtener la distribución  a posteriori de probabilidad $\\beta_k$ de las palabras por cada tema, y la proporción de temas por documento $\\theta_d$:\n",
    "\n",
    "$$ p(\\beta_k|\\mathbf{X}), \\qquad k=1, \\ldots,K$$\n",
    "\n",
    "$$ p(\\theta_d|\\mathbf{X}), \\qquad d=1,\\ldots N$$\n",
    "\n",
    "Para obtener muestras de las distribuciones a posteriori anteriores se hace uso de técnicas de Monte Carlo  basadas en cadenas de Markov.\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~olmos/BBVA/LDA2.png' width=1000 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LDA con Gensim\n",
    "\n",
    "Gensim proporciona una librería para entrenar un modelo [LDA](https://radimrehurek.com/gensim/models/ldamulticore.html) de forma online (podemos reentrenar a medida que incorporamos más documentos al dataset) y de forma distribuida, aprovechando todos los cores CPU para paralelizar y acelerar el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus_bow, num_topics=6, id2word = D, passes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a imprimir las 10 palabras más probables en cada tópico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics():\n",
    "    print(\"Topic: %d \\nWords: %s\" %(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=20,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False,num_words=20)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20,15), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y del siguiente modo calculamos la proporción de cada tópico por documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_document = 4000 #Índice del documento\n",
    "\n",
    "print(recent_nips.iloc[idx_document]['Title_abstract'])\n",
    "\n",
    "for index, score in lda_model[corpus_bow[idx_document]]:\n",
    "    print(\"\\nScore: %f\\t \\n Topic: %r\" %(score, lda_model.print_topic(index, 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, con la siguiente función resumimos la proporción de temas para todos los artículos del dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a pasar a una matriz las proporciones de temas por documento\n",
    "\n",
    "topics_doc = lda_model[corpus_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2dense\n",
    "\n",
    "corpus_LDA_dense = corpus2dense(topics_doc, num_terms=6, num_docs=len(topics_doc)).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_LDA_dense.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección del número de tópicos\n",
    "\n",
    "Al tratarse de modelos no supervisados, no disponemos de una métrica unívoca para la validación de parámetros de un modelo de temas (típicamente el número de temas). En este [articulo](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf) se hace un resumen muy completo de las distintas métricas en un modelo de temas que típicamente se utilizan de forma combinada. Por una parte, tenemos **métricas de ajuste** del modelo a datos de validación:\n",
    "\n",
    "* **LL: log-likelihood**: dado un documento con palabras $\\mathbf{w}_d$, log-probabilidad de ser una muestra del modelo LDA entrenado:\n",
    "\n",
    "$$\\log p(\\mathbf{w}_d) = \\log \\int p(\\theta_d)\\prod_{k=1}^{K}p(\\beta_k)\\prod_{j=1}^{D_d} p(w_{jd}|\\beta_{z_{jd}})p(z_{jd}|\\theta_d)~d\\beta_1\\ldots d\\beta_k d\\theta_d d\\mathbf{z}_d$$\n",
    "\n",
    "* **LP: log_perplexity**: dado un conjunto de validación $\\mathcal{D}_{\\text {val}} =\\left(\\mathbf{w}_1,\\ldots,\\mathbf{w}_M\\right)$, siendo $N_d$ el número de palabras del documento $d$, se define como\n",
    "\n",
    "$$\\text { perplexity }\\left(\\mathcal{D}_{\\text {val}}\\right)=\\exp \\left\\{-\\frac{\\sum_{d=1}^{M} \\log p\\left(\\mathbf{w}_{d}\\right)}{\\sum_{d=1}^{M} N_{d}}\\right\\}$$\n",
    "\n",
    "$\\quad$ A menor valor, mejor ajuste a los datos. **Nota:** `topic_model.log_perplexity` devuelve una cota inferior a $\\sum_{d=1}^{M} \\log p\\left(\\mathbf{w}_{d}\\right)$, por tanto a mayor valor mejor ajuste.\n",
    "\n",
    "Si bien estas métricas están relacionadas con el ajuste de los datos al modelo probabilístico de LDA, existen **métricas de coherencia** que se han demostrado más correladas con un criterio manual (una persona validando la interpretabilidad/coherencia de los temas encontrados). Estas métricas parten de tomar las $T$ palabras más relevantes por tema y promedia una medida de coherencia entre todos los posibles pares en este conjunto de $T$ palabras. A mayor valor, más coherencia interna en los temas:\n",
    "\n",
    "* **UCI coherence score**: información mutua puntual (pointwise mutual information PMI) promedio entre los pares de palabras\n",
    "\n",
    "$$C_{\\mathrm{UCI}}=\\frac{2}{T \\cdot(T-1)} \\sum_{i=1}^{T-1} \\sum_{j=i+1}^{T} \\operatorname{PMI}\\left(w_{i}, w_{j}\\right), \\quad \\operatorname{PMI}\\left(w_{i}, w_{j}\\right)=\\log \\frac{P\\left(w_{i}, w_{j}\\right)+\\epsilon}{P\\left(w_{i}\\right) \\cdot P\\left(w_{j}\\right)}$$\n",
    "\n",
    "* **UMass coherence**: probabilidad condicional promedio entre los pares de palabras\n",
    "\n",
    "$$C_{\\mathrm{UMass}}=\\frac{2}{T \\cdot(T-1)} \\sum_{i=2}^{T} \\sum_{j=1}^{i-1} \\log \\frac{P\\left(w_{i}, w_{j}\\right)+\\epsilon}{P\\left(w_{j}\\right)}$$\n",
    "\n",
    "* Finalmente, existen varias métricas basadas en medir la distancia (e.g. distancia coseno) entre vectores de contexto asociados a cada una de las $T$ palabras. Una elección habitual del vector de contexto es utilizar PMI normalizada en palabras que aparecen en una ventana de $\\pm5$ palabras. Para la palabra $w_i$ de las más relevantes del tema, construiremos el vector \n",
    "$\\mathbf{v}_i$ de dimensión igual al diccionario, de forma que el elemento $j$ del vector $\\mathbf{v}_i$ asociado a la palabra $w_i$ se calcula como\n",
    "$$v_{i j}=\\operatorname{NPMI}\\left(w_{i}, w_{j}\\right)^{\\gamma}=\\left(\\frac{\\log \\frac{P\\left(w_{i}, w_{j}\\right)+\\epsilon}{P\\left(w_{i}\\right) \\cdot P\\left(w_{j}\\right)}}{\\log \\left(P\\left(w_{i}, w_{j}\\right)+\\epsilon\\right)}\\right)^{\\gamma}, \\qquad \\gamma>0$$\n",
    "\n",
    "Finalmente, dados los vectores contexto $\\mathbf{v}_1,\\ldots,\\mathbf{v}_T$ calculamos la métrica de coherencia\n",
    "\n",
    "$$C_{NPMI} = \\frac{2}{T \\cdot(T-1)} \\sum_{i=1}^{T-1} \\sum_{j=i+1}^{T} \\text{dcos}(\\mathbf{v}_i,\\mathbf{v}_j)$$\n",
    "\n",
    "Para el cálculo de las métricas anteriores, las probabilidades $P\\left(w_{i}, w_{j}\\right)$ y $P\\left(w_{i}\\right)$ se calculan a patir de una matriz de co-ocurrencias basada en una **ventana deslizante de tamaño $W$ sobre los textos del conjunto de validación**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el dataset anterior, vamos a generar un conjunto de entrenamiento/validación para ajustar el número $K$ de temas de LDA. A continuación usaremos la clase clase [`models.coherencemodel`](https://radimrehurek.com/gensim/models/coherencemodel.html) nos permite calcular las métricas anteriores de forma sencilla. A esta clase debemos pasarle los documentos de validación **sin vectorizar con BoW!!**, es necesario el texto natural para el cálculo de la matriz de co-ocurrencias en una ventana deslizante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "bow_train = corpus_bow\n",
    "\n",
    "bow_val = corpus_bow_val\n",
    "\n",
    "papers_train = papers_norm\n",
    "\n",
    "papers_val = papers_norm_val\n",
    "\n",
    "K_max = 10\n",
    "\n",
    "L_UCI = []\n",
    "\n",
    "L_mass = []\n",
    "\n",
    "L_NPMI = []\n",
    "\n",
    "L_perp = []\n",
    "\n",
    "L_UCI_train = []\n",
    "\n",
    "L_mass_train = []\n",
    "\n",
    "L_NPMI_train = []\n",
    "\n",
    "L_perp_train = []\n",
    "\n",
    "topic_models = {}\n",
    "\n",
    "\n",
    "for k in range(4,K_max,2):\n",
    "    \n",
    "    print(k)\n",
    "    \n",
    "    topic_models[k] = gensim.models.LdaMulticore(bow_train, num_topics=k, id2word = D, passes=30) # Utilizamos BOW en lugar de TF-IDF!\n",
    "    \n",
    "    #### Perplexity\n",
    "    \n",
    "    L_perp.append(topic_models[k].log_perplexity(bow_val))\n",
    "    \n",
    "    L_perp_train.append(topic_models[k].log_perplexity(bow_train))\n",
    "    \n",
    "    #### UCI\n",
    "    \n",
    "    cm = CoherenceModel(model=topic_models[k], texts=papers_val, dictionary=D, coherence='c_uci')\n",
    "    \n",
    "    L_UCI.append(cm.get_coherence())\n",
    "    \n",
    "    cm = CoherenceModel(model=topic_models[k], texts=papers_train, dictionary=D, coherence='c_uci')\n",
    "    \n",
    "    L_UCI_train.append(cm.get_coherence())    \n",
    "    \n",
    "    #### UMASS\n",
    "    \n",
    "    cm = CoherenceModel(model=topic_models[k], texts=papers_val, dictionary=D, coherence='u_mass')\n",
    "    \n",
    "    L_mass.append(cm.get_coherence())\n",
    "    \n",
    "    cm = CoherenceModel(model=topic_models[k], texts=papers_train, dictionary=D, coherence='u_mass')\n",
    "    \n",
    "    L_mass_train.append(cm.get_coherence())\n",
    "    \n",
    "    #### CNPMI   \n",
    "    \n",
    "    cm = CoherenceModel(model=topic_models[k], texts=papers_val, dictionary=D, coherence='c_npmi')\n",
    "    \n",
    "    L_NPMI.append(cm.get_coherence())\n",
    "    \n",
    "    cm = CoherenceModel(model=topic_models[k], texts=papers_train, dictionary=D, coherence='c_npmi')\n",
    "    \n",
    "    L_NPMI_train.append(cm.get_coherence())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12,16))\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "axes[0,0].plot(range(4,K_max,2),L_perp,color=cols[0],label='Validación')\n",
    "axes[0,0].plot(range(4,K_max,2),L_perp_train,'--',color=cols[0],label='Entrenamiento')\n",
    "axes[0,0].set_ylabel('log-perplexity')\n",
    "axes[0,0].set_xlabel('Número de temas')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid()\n",
    "\n",
    "axes[0,1].plot(range(4,K_max,2),L_UCI,color=cols[1],label='Validación')\n",
    "axes[0,1].plot(range(4,K_max,2),L_UCI_train,'--',color=cols[1],label='Entrenamiento')\n",
    "axes[0,1].set_ylabel('UCI score')\n",
    "axes[0,1].set_xlabel('Número de temas')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid()\n",
    "\n",
    "axes[1,0].plot(range(4,K_max,2),L_mass,color=cols[2],label='Validación')\n",
    "axes[1,0].plot(range(4,K_max,2),L_mass_train,'--',color=cols[2],label='Entrenamiento')\n",
    "axes[1,0].set_ylabel('UMass score')\n",
    "axes[1,0].set_xlabel('Número de temas')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid()\n",
    "\n",
    "axes[1,1].plot(range(4,K_max,2),L_NPMI,color=cols[2],label='Validación')\n",
    "axes[1,1].plot(range(4,K_max,2),L_NPMI_train,'--',color=cols[2],label='Validación')\n",
    "axes[1,1].set_ylabel('Vector NPMI score')\n",
    "axes[1,1].set_xlabel('Número de temas')\n",
    "axes[1,1].legend() \n",
    "axes[1,1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización del modelo de tópicos con pyLDAvis\n",
    "\n",
    "Vamos a visualizar el mejor modelo encontrado con la librería [pyLDAvis](https://github.com/bmabey/pyLDAvis):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pyLDAvis.gensim.prepare(topic_models[8], corpus_bow, D)\n",
    "pyLDAvis.save_html(data, \"LDAvis.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La visualización de pyLDAvis nos ofrece la siguiente información:\n",
    "\n",
    "* **Intertopic distance map (panel de la izquierda)** visualización 2D de los temas. El área por tema es proporcional al **número de palabras que pertenecen a cada tema** en nuestro corpus. Recordad que en el modelo generativo cada palabra se muestrea de un tema en particular, durante la inferencia LDA obtenemos el tema asociado a cada palabra.  Además, los circulos se dibujan en un espacio 2D usando el algoritmo [multidimensional scaling](https://en.wikipedia.org/wiki/Multidimensional_scaling), que proyecta la distribución de cada tema $\\beta_k$, $k=1,\\ldots,K$ en 2D **manteniendo aproximadamente las distancias entre puntos**.\n",
    "\n",
    "* **Palabras más importantes del corpus**, definidas como aquellas que maximizan el siguiente *saliency score*:\n",
    "\n",
    "$$s(w) = p(w) * \\sum_{k=0}^{K} p(k|w) \\log(\\frac{ p(k|w)}{p(k)})$$\n",
    "\n",
    "obsérvese que cada uno de los términos del sumatorio representan la divergencia KL entre la distribución a posteriori de los temas dada la palabra $w$ ($p(k|w)$) y la probabilidad a priori de los mismos ($p(k)$). \n",
    "\n",
    "* **Palabras más relevantes por tema**. Si $\\beta_{k,w}=p(w|k)$ es la probabilidad de la palabra $w$ en el tema $k$, se calcula su relevancia como \n",
    "\n",
    "$$r(w,k|\\lambda) = \\lambda *\\log(\\beta_{k,w}) + (1-\\lambda) \\log(\\frac{\\beta_{k,w}}{p(w)})$$\n",
    "\n",
    "donde $p(w)$ es la probabilidad de palabra en el corpus. Para $\\lambda=1$ ordenamos las palabras según su probabilidad en el tema, para $\\lambda=0$, penalizamos aquellas palabras más probables en el corpus. Esto puede ayudar a interpretar mejor cada tema al filtrar palabras que aparecen en varios temas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
